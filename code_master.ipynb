{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},

   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},

   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [

      "Using device:  /device:GPU:0\n"

      "Using device:  /cpu:0\n"

     ]
    }
   ],
   "source": [

    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import scipy\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import CNet\n",
    "import provider\n",
    "import TR\n",
    "#%matplotlib inline\n",

    "USE_GPU = False\n",

    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n",
    "# Constant to control how often we print when training models\n",
    "print_every = 100\n",
    "\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0930 10:37:15.801015 139676561807296 deprecation_wrapper.py:119] From /home/nj/文档/NJ/centralnet_code/TR.py:15: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W0930 10:37:15.891801 139676561807296 deprecation_wrapper.py:119] From /home/nj/文档/NJ/centralnet_code/CNet.py:248: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0930 10:37:15.898178 139676561807296 deprecation_wrapper.py:119] From /home/nj/文档/NJ/centralnet_code/CNet.py:14: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.\n",
      "\n",
      "W0930 10:37:15.900449 139676561807296 deprecation_wrapper.py:119] From /home/nj/文档/NJ/centralnet_code/CNet.py:24: The name tf.layers.MaxPooling2D is deprecated. Please use tf.compat.v1.layers.MaxPooling2D instead.\n",
      "\n",
      "W0930 10:37:15.906658 139676561807296 deprecation_wrapper.py:119] From /home/nj/文档/NJ/centralnet_code/CNet.py:94: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
      "\n",
      "W0930 10:37:17.697125 139676561807296 deprecation_wrapper.py:119] From /home/nj/文档/NJ/centralnet_code/CNet.py:260: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "W0930 10:37:17.822900 139676561807296 deprecation_wrapper.py:119] From /home/nj/文档/NJ/centralnet_code/TR.py:34: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "W0930 10:37:17.823911 139676561807296 deprecation_wrapper.py:119] From /home/nj/文档/NJ/centralnet_code/TR.py:34: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W0930 10:37:18.293279 139676561807296 deprecation_wrapper.py:119] From /home/nj/文档/NJ/centralnet_code/TR.py:39: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1994e0811b22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mTR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_part34\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_init_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_init_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/文档/NJ/centralnet_code/TR.py\u001b[0m in \u001b[0;36mtrain_part34\u001b[0;34m(model_init_fn, optimizer_init_fn, MAX_EPOCH, device, BATCH_SIZE)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m#        print()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m#    t += 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0meval_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meval_acc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0meval_acc_max\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/文档/NJ/centralnet_code/TR.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(sess, ops, data_input, BATCH_SIZE)\u001b[0m\n\u001b[1;32m    102\u001b[0m                                          \u001b[0mops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                                          ops['scores']],\n\u001b[0;32m--> 104\u001b[0;31m                                         feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mloss_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_a_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_s_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_a_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_s_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/demo_central3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/demo_central3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/demo_central3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/demo_central3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/demo_central3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/demo_central3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=8\n",
    "MAX_EPOCH=1\n",
    "learning_rate = 3e-5\n",
    "channel_1, channel_2,channel_3,channel_4, num_classes,c1,c2,c3,c4=64,128,256,512,2,64,128,256,512\n",

   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,) int32\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n",
      "0 (64, 32, 32, 3) (64,)\n",
      "1 (64, 32, 32, 3) (64,)\n",
      "2 (64, 32, 32, 3) (64,)\n",
      "3 (64, 32, 32, 3) (64,)\n",
      "4 (64, 32, 32, 3) (64,)\n",
      "5 (64, 32, 32, 3) (64,)\n",
      "6 (64, 32, 32, 3) (64,)\n"
     ]
    }
   ],
   "source": [
    "#准备数据\n",
    "\n",
    "def load_cifar10(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Fetch the CIFAR-10 dataset from the web and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 dataset and use appropriate data types and shapes\n",
    "    cifar10 = tf.keras.datasets.cifar10.load_data()\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.int32).flatten()\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "    y_test = np.asarray(y_test, dtype=np.int32).flatten()\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean pixel and divide by std\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "NHW = (0, 1, 2)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
    "\n",
    "\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=64)\n",
    "\n",
    "# We can iterate through a dataset like this:\n",
    "for t, (x, y) in enumerate(train_dset):\n",
    "    print(t, x.shape, y.shape)\n",
    "    if t > 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(sess, dset, x1,x2, scores, is_training=None):\n",
    "    \"\"\"\n",
    "    Check accuracy on a classification model.\n",
    "    \n",
    "    Inputs:\n",
    "    - sess: A TensorFlow Session that will be used to run the graph\n",
    "    - dset: A Dataset object on which to check accuracy\n",
    "    - x: A TensorFlow placeholder Tensor where input images should be fed\n",
    "    - scores: A TensorFlow Tensor representing the scores output from the\n",
    "      model; this is the Tensor we will ask TensorFlow to evaluate.\n",
    "      \n",
    "    Returns: Nothing, but prints the accuracy of the model\n",
    "    \"\"\"\n",
    "    num_correct, num_samples = 0, 0\n",
    "    for x_batch, y_batch in dset:\n",
    "        feed_dict = {x1: x_batch,x2:x_batch, is_training: 0}\n",
    "        scores_np = sess.run(scores, feed_dict=feed_dict)\n",
    "        y_pred = scores_np.argmax(axis=1)\n",
    "        num_samples += x_batch.shape[0]\n",
    "        num_correct += (y_pred == y_batch).sum()\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CentralNet(tf.keras.Model):\n",
    "    def __init__(self, channel_1, channel_2, num_classes,c1):\n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Implement the __init__ method for a three-layer ConvNet. You   #\n",
    "        # should instantiate layer objects to be used in the forward pass.     #\n",
    "        ########################################################################\n",
    "#         pass\n",
    "        initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "        self.conv1 = tf.layers.Conv2D(channel_1, kernel_size=(5,5), \n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer())\n",
    "        self.conv2 = tf.layers.Conv2D(channel_2, kernel_size=(3,3), \n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer())\n",
    "        self.fc4c1 = tf.layers.Dense(c1, activation=None,use_bias=False,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=tf.zeros_initializer())\n",
    "        self.fc4c2 = tf.layers.Dense(num_classes, activation=None,use_bias=False,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=tf.zeros_initializer())\n",
    "        self.fc4cc = tf.layers.Dense(num_classes, activation=None,use_bias=False,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=tf.zeros_initializer())\n",
    "        self.fc4conv = tf.layers.Dense(num_classes, activation=None,use_bias=True,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=tf.zeros_initializer())\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "                                      \n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "\n",
    "    \n",
    "    def call(self, x1,x2, training=None):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward pass for a three-layer ConvNet. You      #\n",
    "        # should use the layer objects defined in the __init__ method.         #\n",
    "        ########################################################################\n",
    "#         pass\n",
    "        x1_conv1 = self.conv1(x1)\n",
    "        h1=self.flatten(x1_conv1)\n",
    "        x2_conv1 = self.conv1(x2)\n",
    "        h2=self.flatten(x2_conv1)\n",
    "        hc1=self.fc4c1(h1)+self.fc4c1(h2)\n",
    "        ##\n",
    "        x1_conv2 = self.conv2(x1_conv1)\n",
    "        x2_conv2 = self.conv2(x2_conv1)\n",
    "        x1_flat = self.flatten(x1_conv2)\n",
    "        x2_flat = self.flatten(x2_conv2)\n",
    "        hc2=self.fc4c2(x1_flat)+self.fc4c2(x2_flat)+self.fc4cc(hc1)\n",
    "        scores = self.fc4conv(x1_flat)+self.fc4conv(x2_flat)+hc2\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def test_CentralNet():\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    channel_1, channel_2, num_classes,c1 = 12, 8, 10, 4\n",
    "    model = CentralNet(channel_1, channel_2, num_classes,c1)\n",
    "    with tf.device(device):\n",
    "        x1 = tf.zeros((64, 3, 32, 32))\n",
    "        x2 = tf.zeros((64, 3, 32, 32))\n",
    "        scores = model(x1,x2)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        scores_np = sess.run(scores)\n",
    "        print(scores_np.shape)\n",
    "\n",
    "test_CentralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
    "    accuracy on the CIFAR-10 validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints progress during trainingn\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()    \n",
    "    with tf.device(device):\n",
    "        # Construct the computational graph we will use to train the model. We\n",
    "        # use the model_init_fn to construct the model, declare placeholders for\n",
    "        # the data and labels\n",
    "        x1 = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        x2 = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        # We need a place holder to explicitly specify if the model is in the training\n",
    "        # phase or not. This is because a number of layers behaves differently in\n",
    "        # training and in testing, e.g., dropout and batch normalization.\n",
    "        # We pass this variable to the computation graph through feed_dict as shown below.\n",
    "        is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "        \n",
    "        # Use the model function to build the forward pass.\n",
    "        scores = model_init_fn(x1,x2, is_training)\n",
    "\n",
    "        # Compute the loss like we did in Part II\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        # Use the optimizer_fn to construct an Optimizer, then use the optimizer\n",
    "        # to set up the training step. Asking TensorFlow to evaluate the\n",
    "        # train_op returned by optimizer.minimize(loss) will cause us to make a\n",
    "        # single update step using the current minibatch of data.\n",
    "        \n",
    "        # Note that we use tf.control_dependencies to force the model to run\n",
    "        # the tf.GraphKeys.UPDATE_OPS at each training step. tf.GraphKeys.UPDATE_OPS\n",
    "        # holds the operators that update the states of the network.\n",
    "        # For example, the tf.layers.batch_normalization function adds the running mean\n",
    "        # and variance update operators to tf.GraphKeys.UPDATE_OPS.\n",
    "        optimizer = optimizer_init_fn()\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = optimizer.minimize(loss)\n",
    "\n",
    "    # Now we can run the computational graph many times to train the model.\n",
    "    # When we call sess.run we ask it to evaluate train_op, which causes the\n",
    "    # model to update.\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        t = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Starting epoch %d' % epoch)\n",
    "            for x_np, y_np in train_dset:\n",
    "                feed_dict = {x1: x_np,x2:x_np, y: y_np, is_training:1}\n",
    "                loss_np, _ = sess.run([loss, train_op], feed_dict=feed_dict)\n",
    "                if t % print_every == 0:\n",
    "                    print('Iteration %d, loss = %.4f' % (t, loss_np))\n",
    "                    check_accuracy(sess, val_dset, x1,x2, scores, is_training=is_training)\n",
    "                    print()\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 7.4852\n",
      "Got 68 / 1000 correct (6.80%)\n",
      "\n",
      "Iteration 100, loss = 3.6246\n",
      "Got 157 / 1000 correct (15.70%)\n",
      "\n",
      "Iteration 200, loss = 3.6126\n",
      "Got 204 / 1000 correct (20.40%)\n",
      "\n",
      "Iteration 300, loss = 3.3209\n",
      "Got 207 / 1000 correct (20.70%)\n",
      "\n",
      "Iteration 400, loss = 3.0245\n",
      "Got 224 / 1000 correct (22.40%)\n",
      "\n",
      "Iteration 500, loss = 3.5126\n",
      "Got 231 / 1000 correct (23.10%)\n",
      "\n",
      "Iteration 600, loss = 3.1422\n",
      "Got 238 / 1000 correct (23.80%)\n",
      "\n",
      "Iteration 700, loss = 3.1523\n",
      "Got 252 / 1000 correct (25.20%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-5\n",
    "channel_1, channel_2, num_classes,c1 = 32, 16, 10, 4\n",

    "\n",
    "def model_init_fn(inputs1,inputs2, is_training):\n",
    "    model = None\n",
    "\n",

    "    model = CNet.CentralNet(channel_1, channel_2,channel_3,channel_4, num_classes,c1,c2,c3,c4)\n",

    "\n",
    "    return model(inputs1,inputs2)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "    return optimizer\n",
    "\n",

    "TR.train_part34(model_init_fn, optimizer_init_fn,100,device,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('conv2d/bias', [64])\n",
      "('conv2d/kernel', [3, 3, 3, 64])\n",
      "('conv2d_1/bias', [64])\n",
      "('conv2d_1/kernel', [3, 3, 64, 64])\n",
      "('conv2d_10/bias', [512])\n",
      "('conv2d_10/kernel', [3, 3, 512, 512])\n",
      "('conv2d_11/bias', [512])\n",
      "('conv2d_11/kernel', [3, 3, 512, 512])\n",
      "('conv2d_12/bias', [512])\n",
      "('conv2d_12/kernel', [3, 3, 512, 512])\n",
      "('conv2d_2/bias', [128])\n",
      "('conv2d_2/kernel', [3, 3, 64, 128])\n",
      "('conv2d_3/bias', [128])\n",
      "('conv2d_3/kernel', [3, 3, 128, 128])\n",
      "('conv2d_4/bias', [256])\n",
      "('conv2d_4/kernel', [3, 3, 128, 256])\n",
      "('conv2d_5/bias', [256])\n",
      "('conv2d_5/kernel', [3, 3, 256, 256])\n",
      "('conv2d_6/bias', [256])\n",
      "('conv2d_6/kernel', [3, 3, 256, 256])\n",
      "('conv2d_7/bias', [512])\n",
      "('conv2d_7/kernel', [3, 3, 256, 512])\n",
      "('conv2d_8/bias', [512])\n",
      "('conv2d_8/kernel', [3, 3, 512, 512])\n",
      "('conv2d_9/bias', [512])\n",
      "('conv2d_9/kernel', [3, 3, 512, 512])\n",
      "('dense/kernel', [211200, 64])\n",
      "('dense_1/kernel', [108800, 128])\n",
      "('dense_10/bias', [1000])\n",
      "('dense_10/kernel', [4096, 1000])\n",
      "('dense_11/bias', [2])\n",
      "('dense_11/kernel', [1000, 2])\n",
      "('dense_2/kernel', [64, 128])\n",
      "('dense_3/kernel', [57600, 256])\n",
      "('dense_4/kernel', [128, 256])\n",
      "('dense_5/kernel', [33280, 512])\n",
      "('dense_6/kernel', [256, 512])\n",
      "('dense_7/bias', [4096])\n",
      "('dense_7/kernel', [10752, 4096])\n",
      "('dense_8/kernel', [512, 4096])\n",
      "('dense_9/bias', [4096])\n",
      "('dense_9/kernel', [4096, 4096])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.framework.python.framework import checkpoint_utils\n",
    "var_list = checkpoint_utils.list_variables(\"log/model_best.ckpt\")\n",
    "for v in var_list:\n",
    "    print(v)\n"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
