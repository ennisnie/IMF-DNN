{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lvcxkuA1qu3f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import scipy\n",
    "import laspy\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import provider\n",
    "# import TR\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "neZwg521q5G3"
   },
   "outputs": [],
   "source": [
    "class CentralNet(tf.keras.Model):\n",
    "    def __init__(self, channel_1, channel_2,channel_3,channel_4, num_classes,num_points):\n",
    "        super().__init__()\n",
    "\n",
    "         # vgg16\n",
    "        initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "        self.conv1 = tf.layers.Conv2D(channel_1, kernel_size=(3,3), \n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer())\n",
    "        self.conv2 = tf.layers.Conv2D(channel_1, kernel_size=(3,3), \n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer())\n",
    "        self.maxpool1 = tf.layers.MaxPooling2D(pool_size = (2,2), \n",
    "                               strides = (2,2), padding='SAME')\n",
    "        \n",
    "        self.conv3 = tf.layers.Conv2D(channel_2, kernel_size=(3,3), \n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer())        \n",
    "        self.conv4 = tf.layers.Conv2D(channel_2, kernel_size=(3,3), \n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer())\n",
    "        self.maxpool2 = tf.layers.MaxPooling2D(pool_size = (2,2), \n",
    "                               strides = (2,2), padding='SAME')\n",
    "        \n",
    "        self.conv5 = tf.layers.Conv2D(channel_3, kernel_size=(3,3),\n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer()) # conv3-256\n",
    "        self.conv6 = tf.layers.Conv2D(channel_3, kernel_size=(3,3),\n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer()) # conv3-256\n",
    "        self.conv7 = tf.layers.Conv2D(channel_3, kernel_size=(3,3),\n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer()) # conv3-256\n",
    "        self.maxpool3 = tf.layers.MaxPooling2D(pool_size = (2,2), \n",
    "                               strides = (2,2), padding='SAME')\n",
    "\n",
    "        self.conv8 = tf.layers.Conv2D(channel_4, kernel_size=(3,3),\n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer()) # conv3-512\n",
    "        self.conv9 = tf.layers.Conv2D(channel_4, kernel_size=(3,3),\n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer()) # conv3-512\n",
    "        self.conv10 = tf.layers.Conv2D(channel_4, kernel_size=(3,3),\n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer()) # conv3-512\n",
    "        self.maxpool4 = tf.layers.MaxPooling2D(pool_size = (2,2), \n",
    "                               strides = (2,2), padding='SAME')\n",
    "\n",
    "        self.conv11 = tf.layers.Conv2D(channel_4, kernel_size=(3,3),\n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer()) # conv3-512\n",
    "        self.conv12 = tf.layers.Conv2D(channel_4, kernel_size=(3,3),\n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer()) # conv3-512\n",
    "        self.conv13 = tf.layers.Conv2D(channel_4, kernel_size=(3,3),\n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer()) # conv3-512\n",
    "        self.maxpool5 = tf.layers.MaxPooling2D(pool_size = (2,2), \n",
    "                               strides = (2,2), padding='SAME')\n",
    "        ###pointnet\n",
    "        self.convpn1 = tf.layers.Conv2D(64, kernel_size=(1,3),\n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer()) # conv3-512\n",
    "        self.convpn2 = tf.layers.Conv2D(64, kernel_size=(1,1),\n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer()) # conv3-512\n",
    "        self.convpn3 = tf.layers.Conv2D(64, kernel_size=(1,1),\n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer()) # conv3-512\n",
    "        self.convpn4 = tf.layers.Conv2D(128, kernel_size=(1,1),\n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer()) # conv3-512\n",
    "        self.convpn5 = tf.layers.Conv2D(256, kernel_size=(1,1),\n",
    "                                      strides=(1,1),padding=\"SAME\",\n",
    "                                      activation = tf.nn.relu,use_bias=True,\n",
    "                                      kernel_initializer = initializer,\n",
    "                                      bias_initializer=tf.zeros_initializer()) # conv3-512\n",
    "        self.maxpoolpn = tf.layers.MaxPooling2D(pool_size = (num_points,1),strides=(num_points,1), padding='SAME')\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### centralnet\n",
    "                                     \n",
    "        self.fc4c1 = tf.layers.Dense(channel_1, activation=tf.nn.relu,use_bias=False,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=tf.zeros_initializer())\n",
    "        \n",
    "        self.c14c2 = tf.layers.Dense(channel_2, activation=tf.nn.relu,use_bias=False,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=tf.zeros_initializer())\n",
    "        self.fc4c2 = tf.layers.Dense(channel_2, activation=tf.nn.relu,use_bias=False,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=tf.zeros_initializer())\n",
    "        self.c24c3 = tf.layers.Dense(channel_3, activation=tf.nn.relu,use_bias=False,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=tf.zeros_initializer())\n",
    "        self.fc4c3 = tf.layers.Dense(channel_3, activation=tf.nn.relu,use_bias=False,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=tf.zeros_initializer())\n",
    "        self.c34c4 = tf.layers.Dense(channel_4, activation=tf.nn.relu,use_bias=False,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=tf.zeros_initializer())\n",
    "        self.fc4c4 = tf.layers.Dense(channel_4, activation=tf.nn.relu,use_bias=False,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=tf.zeros_initializer())\n",
    "        self.c44c5 = tf.layers.Dense(4096, activation=tf.nn.relu,use_bias=False,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=tf.zeros_initializer())\n",
    "        \n",
    "        self.fc4p1 = tf.layers.Dense(channel_1, activation=tf.nn.relu,use_bias=False,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=tf.zeros_initializer())\n",
    "        self.fc4p2 = tf.layers.Dense(channel_2, activation=tf.nn.relu,use_bias=False,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=tf.zeros_initializer())\n",
    "        self.fc4p3 = tf.layers.Dense(channel_3, activation=tf.nn.relu,use_bias=False,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=tf.zeros_initializer())\n",
    "        self.fc4p4 = tf.layers.Dense(channel_4, activation=tf.nn.relu,use_bias=False,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=tf.zeros_initializer())\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        self.fc1=tf.layers.Dense(4096, activation = tf.nn.relu, \n",
    "                        kernel_initializer=initializer)\n",
    "        self.fp1=tf.layers.Dense(4096, activation = tf.nn.relu, \n",
    "                        kernel_initializer=initializer)\n",
    "        self.fc2=tf.layers.Dense(4096, activation = tf.nn.relu, \n",
    "                        kernel_initializer=initializer)\n",
    "        self.fc3=tf.layers.Dense(1024, activation = tf.nn.relu, \n",
    "                        kernel_initializer=initializer)\n",
    "        self.fc4=tf.layers.Dense(num_classes, activation = None, \n",
    "                        kernel_initializer=initializer)\n",
    "     \n",
    "    def call(self, x1, point_cloud, training=None):\n",
    "        scores = None\n",
    "        batch_size = point_cloud.get_shape()[0].value\n",
    "        num_point = point_cloud.get_shape()[1].value\n",
    "        x2 = tf.expand_dims(point_cloud, -1)\n",
    "        ###############################################\n",
    "        ###first input x1 \n",
    "        x1_conv1 = self.conv1(x1)\n",
    "        x1_conv2 = self.conv2(x1_conv1)\n",
    "        x1_maxpool1 = self.maxpool1(x1_conv2)\n",
    "        h1_x1=self.flatten(x1_maxpool1) #8,33x100x64\n",
    "        ###second input x2 \n",
    "        pn_conv1 = self.convpn1(x2)\n",
    "        h1_x2=self.flatten(pn_conv1)#8,10000x3x64\n",
    "        ### central net\n",
    "        hc1=self.fc4c1(h1_x1)+self.fc4p1(h1_x2) #(8,64)+(8,64)=（8,64）\n",
    "        ## conv3-64\n",
    "        ##############################################\n",
    "        x1_conv3 = self.conv3(x1_maxpool1)\n",
    "        x1_conv4 = self.conv4(x1_conv3)\n",
    "        x1_maxpool2 = self.maxpool2(x1_conv4)\n",
    "        h2_x1=self.flatten(x1_maxpool2)#(64,16,50,128)\n",
    "        ### \n",
    "        pn_conv2 = self.convpn2(pn_conv1)\n",
    "        \n",
    "        h2_x2=self.flatten(tf.layers.max_pooling2d(pn_conv2,pool_size = (num_points,1),strides=(16,1), padding='SAME')) #8,10000x3x64\n",
    "        ###\n",
    "        hc2=self.fc4c2(h2_x1)+self.fc4p2(h2_x2)+self.c14c2(hc1)#(8,128)+(8,128)+(8,128)\n",
    "        ##conv3-128\n",
    "        #############################################\n",
    "        x1_conv5 = self.conv5(x1_maxpool2)\n",
    "        x1_conv6 = self.conv6(x1_conv5)\n",
    "        x1_conv7 = self.conv7(x1_conv6)\n",
    "        x1_maxpool3 = self.maxpool3(x1_conv7)\n",
    "        h3_x1=self.flatten(x1_maxpool3)\n",
    "        ###\n",
    "        pn_conv3 = self.convpn3(pn_conv2)\n",
    "        h3_x2=self.flatten(tf.layers.max_pooling2d(pn_conv3,pool_size = (num_points,1),strides=(32,1), padding='SAME')) #8,10000x3x64\n",
    "        ###\n",
    "        hc3=self.fc4c3(h3_x1)+self.fc4p3(h3_x2)+self.c24c3(hc2)\n",
    "        ##conv3-256\n",
    "        ##################################################\n",
    "        x1_conv8 = self.conv8(x1_maxpool3)\n",
    "        x1_conv9 = self.conv9(x1_conv8)\n",
    "        x1_conv10 = self.conv10(x1_conv9)\n",
    "        x1_maxpool4 = self.maxpool4(x1_conv10)\n",
    "        h4_x1=self.flatten(x1_maxpool4)\n",
    "        ###\n",
    "        pn_conv4 = self.convpn4(pn_conv3)\n",
    "        h4_x2=self.flatten(tf.layers.max_pooling2d(pn_conv4,pool_size = (num_points,1),strides=(32,1), padding='SAME')) #8,10000x3x128\n",
    "        ###\n",
    "        hc4=self.fc4c4(h4_x1)+self.fc4p4(h4_x2)+self.c34c4(hc3)\n",
    "        ##conv3-512\n",
    "        ##################################################\n",
    "        x1_conv11 = self.conv11(x1_maxpool4)\n",
    "        x1_conv12 = self.conv12(x1_conv11)\n",
    "        x1_conv13 = self.conv13(x1_conv12)\n",
    "        x1_maxpool5 = self.maxpool5(x1_conv13)#64x4x12x512\n",
    "        h5_x1=self.flatten(x1_maxpool5)\n",
    "        ###\n",
    "        pn_conv5 = self.convpn5(pn_conv4) #64x10000x3x256\n",
    "        pn_maxpool= self.maxpoolpn(pn_conv5) #64x1x3x256\n",
    "        h5_x2=self.flatten(pn_maxpool)#8,1x3x256\n",
    "        ###\n",
    "        hc5=self.fc1(h5_x1)+self.fp1(h5_x2)+self.c44c5(hc4)    #x1 and x2 have the same size    #4096\n",
    "        ###################################################\n",
    "        dense2=self.fc2(hc5) #4096\n",
    "        dense3=self.fc3(dense2)#1000\n",
    "        scores= self.fc4(dense3)#num_classes\n",
    "        return scores\n",
    "\n",
    "def placeholder_inputs_pc(batch_size, img_rows=66, img_cols=200, points=16384, separately=False):\n",
    "    imgs_pl = tf.placeholder(tf.float32, shape=(batch_size, img_rows, img_cols, 3))\n",
    "    pts_pl = tf.placeholder(tf.float32, shape=(batch_size, points, 3))\n",
    "    if separately:\n",
    "        speeds_pl = tf.placeholder(tf.float32, shape=(batch_size))\n",
    "        angles_pl = tf.placeholder(tf.float32, shape=(batch_size))\n",
    "        labels_pl = [speeds_pl, angles_pl]\n",
    "    labels_pl = tf.placeholder(tf.float32, shape=(batch_size, 2))\n",
    "    return imgs_pl, pts_pl, labels_pl\n",
    "\n",
    "\n",
    "def get_loss(pred, label, l2_weight=0.0001):\n",
    "    diff = tf.square(tf.subtract(pred, label))\n",
    "    train_vars = tf.trainable_variables()\n",
    "    l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in train_vars[1:]]) * l2_weight\n",
    "    loss = tf.reduce_mean(diff + l2_loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1139,
     "status": "ok",
     "timestamp": 1591192022515,
     "user": {
      "displayName": "Ennis Nie",
      "photoUrl": "",
      "userId": "03501380782397734828"
     },
     "user_tz": -480
    },
    "id": "OBf0z4nKbtGr",
    "outputId": "06614d35-7dc9-4e12-9555-283ef4d9910f"
   },
   "outputs": [],
   "source": [
    "USE_GPU = False\n",
    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n",
    "# Constant to control how often we print when training models\n",
    "print_every = 100\n",
    "\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20873,
     "status": "ok",
     "timestamp": 1591189057454,
     "user": {
      "displayName": "Ennis Nie",
      "photoUrl": "",
      "userId": "03501380782397734828"
     },
     "user_tz": -480
    },
    "id": "fHHlmLf6rKsq",
    "outputId": "4e395459-3ba0-41ee-e86c-c07c4f48486a"
   },
   "outputs": [],
   "source": [
    "### testing network\n",
    "### if the final output is (8,10), means the network is workable\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "channel_1, channel_2,channel_3,channel_4, num_classes,num_points=64,128,256,512,10,16384\n",
    "model = CentralNet(channel_1, channel_2,channel_3,channel_4, num_classes,num_points)\n",
    "with tf.device(device):\n",
    "    x1 = tf.zeros((8, 66, 200,3))\n",
    "    x2 = tf.zeros((8, num_points,3))\n",
    "    scores = model(x1,x2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    scores_np = sess.run(scores)\n",
    "    print(scores_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rAfzI8fTSohN"
   },
   "outputs": [],
   "source": [
    "### training\n",
    "\n",
    "def train_part34(model_init_fn, optimizer_init_fn, MAX_EPOCH, device, BATCH_SIZE):\n",
    "\n",
    "    tf.reset_default_graph()    \n",
    "    with tf.device(device):\n",
    "\n",
    "        data_input = provider.DVR_Points_Provider()\n",
    "\n",
    "        imgs_pl, pts_pl, labels_pl = placeholder_inputs_pc(BATCH_SIZE)\n",
    "        x1=imgs_pl\n",
    "        x2=pts_pl\n",
    "        y=labels_pl\n",
    "        \n",
    "        is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "        \n",
    "        # Use the model function to build the forward pass.\n",
    "        scores = model_init_fn(x1,x2, is_training)\n",
    "\n",
    "        # Compute the loss like we did in Part II\n",
    "        loss = get_loss(scores,y)\n",
    "        \n",
    "        optimizer = optimizer_init_fn()\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = optimizer.minimize(loss)\n",
    "        \n",
    "        # Add ops to save and restore all the variables\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "            \n",
    "\n",
    "    # Now we can run the computational graph many times to train the model.\n",
    "    # When we call sess.run we ask it to evaluate train_op, which causes the\n",
    "    # model to update.\n",
    "    # Create a session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.allow_soft_placement = True\n",
    "    config.log_device_placement = False\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    ops = {'x1': x1,\n",
    "            'x2': x2,\n",
    "            'y': y,\n",
    "            'is_training': is_training,\n",
    "            'train_op': train_op,\n",
    "            'scores': scores,\n",
    "            'loss': loss}\n",
    "\n",
    "    eval_acc_max = 0\n",
    "    for epoch in range(MAX_EPOCH):\n",
    "        print('Starting epoch %d' % epoch)\n",
    "        #for x_np, y_np in train_dset:\n",
    "        #    feed_dict = {x1: x_np,x2:x_np, y: y_np, is_training:1}\n",
    "        #    loss_np, _ = sess.run([loss, train_op], feed_dict=feed_dict)\n",
    "        #    if t % print_every == 0:\n",
    "        #        print('Iteration %d, loss = %.4f' % (t, loss_np))\n",
    "        #        check_accuracy(sess, val_dset, x1,x2, scores, is_training=is_training)\n",
    "        #        print()\n",
    "        #    t += 1\n",
    "        train_one_epoch(sess, ops, data_input, BATCH_SIZE)\n",
    "        eval_acc=eval_one_epoch(sess, ops, data_input, BATCH_SIZE)\n",
    "        if eval_acc > eval_acc_max:\n",
    "            eval_acc_max = eval_acc\n",
    "            save_path = saver.save(sess, \"log/model_best.ckpt\")\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "        # Save the variables to disk.\n",
    "        if epoch % 10 == 0:\n",
    "            save_path = saver.save(sess, 'log/model.ckpt')\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "def train_one_epoch(sess, ops, data_input, BATCH_SIZE):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = True\n",
    "    num_batches = data_input.num_train // BATCH_SIZE\n",
    "    loss_sum = 0\n",
    "    acc_a_sum = 0\n",
    "    acc_s_sum = 0\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "\n",
    "        imgs, pts,labels = data_input.load_one_batch(BATCH_SIZE, \"train\")\n",
    "        feed_dict = {ops['x1']: imgs,\n",
    "                     ops['x2']: pts,\n",
    "                     ops['y']: labels,\n",
    "                     ops['is_training']: is_training}\n",
    "\n",
    "        _, loss_val, pred_val = sess.run([ops['train_op'],\n",
    "                                         ops['loss'],\n",
    "                                         ops['scores']],\n",
    "                                        feed_dict=feed_dict)\n",
    "\n",
    "        loss_sum, acc_a_sum, acc_s_sum=check_accuracy(loss_sum, acc_a_sum, acc_s_sum,pred_val,labels)\n",
    "\n",
    "    print('mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "    print('accuracy (angle): %f' % (acc_a_sum / float(num_batches)))\n",
    "    print('accuracy (speed): %f' % (acc_s_sum / float(num_batches)))\n",
    "\n",
    "\n",
    "def eval_one_epoch(sess, ops, data_input, BATCH_SIZE):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = False\n",
    "    loss_sum = 0\n",
    "\n",
    "    num_batches = data_input.num_val // BATCH_SIZE\n",
    "    loss_sum = 0\n",
    "    acc_a_sum = 0\n",
    "    acc_s_sum = 0\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        imgs, pts,labels = data_input.load_one_batch(BATCH_SIZE, \"val\")\n",
    "        feed_dict = {ops['x1']: imgs,\n",
    "                     ops['x2']: pts,\n",
    "                     ops['y']: labels,\n",
    "                     ops['is_training']: is_training}\n",
    "\n",
    "        _, loss_val, pred_val = sess.run([ops['train_op'],\n",
    "                                         ops['loss'],\n",
    "                                         ops['scores']],\n",
    "                                        feed_dict=feed_dict)\n",
    "        \n",
    "        loss_sum, acc_a_sum, acc_s_sum=check_accuracy(loss_sum, acc_a_sum, acc_s_sum,pred_val,labels)\n",
    "\n",
    "    print('eval mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "    print('eval accuracy (angle): %f' % (acc_a_sum / float(num_batches)))\n",
    "    print('eval accuracy (speed): %f' % (acc_s_sum / float(num_batches)))\n",
    "    return acc_a_sum / float(num_batches)\n",
    "\n",
    "def  check_accuracy(loss_sum, acc_a_sum, acc_s_sum,pred_val,labels):\n",
    "    loss_sum += np.mean(np.square(np.subtract(pred_val, labels)))\n",
    "    acc_a = np.abs(np.subtract(pred_val[:, 1], labels[:, 1])) < (5.0 / 180 * scipy.pi)\n",
    "    acc_a = np.mean(acc_a)\n",
    "    acc_a_sum += acc_a\n",
    "    acc_s = np.abs(np.subtract(pred_val[:, 0], labels[:, 0])) < (5.0 / 20)\n",
    "    acc_s = np.mean(acc_s)\n",
    "    acc_s_sum += acc_s\n",
    "    return loss_sum, acc_a_sum, acc_s_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 88767,
     "status": "error",
     "timestamp": 1591192118400,
     "user": {
      "displayName": "Ennis Nie",
      "photoUrl": "",
      "userId": "03501380782397734828"
     },
     "user_tz": -480
    },
    "id": "-23GpggvSth9",
    "outputId": "8b28f20d-2cd3-428f-f098-5e6dcd70d53d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=4\n",
    "MAX_EPOCH=1\n",
    "learning_rate = 1e-8\n",
    "channel_1, channel_2,channel_3,channel_4, num_classes,num_points=64,128,256,512,2,16384\n",
    "\n",
    "def model_init_fn(inputs1,inputs2, is_training):\n",
    "    model = None\n",
    "\n",
    "    model = CentralNet(channel_1, channel_2,channel_3,channel_4, num_classes,num_points)\n",
    "\n",
    "    return model(inputs1,inputs2)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "    return optimizer\n",
    "# train_part34(model_init_fn, optimizer_init_fn, MAX_EPOCH, device, BATCH_SIZE)\n",
    "train_part34(model_init_fn, optimizer_init_fn,2,device,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sHqGr_X0S4i5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMG7EaEMW5ZwfMszc1RqShb",
   "collapsed_sections": [],
   "name": "image_pointnet_fusion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
