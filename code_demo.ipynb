{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n",
    "# Constant to control how often we print when training models\n",
    "print_every = 100\n",
    "\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important block\n",
    "def flatten(x):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    - TensorFlow Tensor of shape (N, D1, ..., DM)\n",
    "    \n",
    "    Output:\n",
    "    - TensorFlow Tensor of shape (N, D1 * ... * DM)\n",
    "    \"\"\"\n",
    "    N = tf.shape(x)[0]\n",
    "    return tf.reshape(x, (N, -1))\n",
    "    \n",
    "def fc(x, w):\n",
    "\n",
    "    x = flatten(x)   # Flatten the input; now x has shape (N, D)\n",
    "    ly=tf.matmul(x, w)\n",
    "    out = tf.nn.relu(ly) # Hidden layer: h has shape (N, H)\n",
    "    return out\n",
    "\n",
    "def convlayer(x, params):\n",
    "    conv_w, conv_b = params\n",
    "    x_conv = tf.nn.conv2d(x, conv_w, [1,1,1,1], \"SAME\") \n",
    "    x_conv_b = tf.nn.bias_add(x_conv,conv_b) \n",
    "    out = tf.nn.relu(x_conv_b)\n",
    "    return out\n",
    "\n",
    "def three_layer_convnet(x, params):\n",
    "    conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b = params\n",
    "    scores = None\n",
    "    ly1=convlayer(x,[conv_w1,conv_b1])\n",
    "    ly2=convlayer(ly1,[conv_w2,conv_b2])\n",
    "    x_flat = flatten(ly2)\n",
    "    scores = tf.matmul(x_flat, fc_w) + fc_b\n",
    "\n",
    "    return ly1,ly2,scores\n",
    "\n",
    "def centralnet(h1,h2,params):\n",
    "    w1,w2=params\n",
    "    h1=flatten(h1) #h1,h2 of shape(N,D)\n",
    "    h2=flatten(h2)\n",
    "    hc=tf.matmul(h1,w1)+tf.matmul(h2,w2)#Hidden layer:hc has shape (N,H)\n",
    "    return hc\n",
    "\n",
    "def conv_centralnet(x1,x2,params):\n",
    "    feed1,feed2,feedc=params\n",
    "    #convnet\n",
    "    h11,h21,out1 = three_layer_convnet(x1, feed1)\n",
    "    h12,h22,out2 = three_layer_convnet(x2, feed2)\n",
    "\n",
    "    #centralnet\n",
    "    c11,c12,c21,c22,cc1=feedc\n",
    "    hc1=centralnet(h11,h12,[c11,c12])\n",
    "    hc2=centralnet(h21,h22,[c21,c22])+tf.matmul(hc1,cc1)\n",
    "\n",
    "    #final\n",
    "    scores= out1+out2+hc2\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_np has shape:  (64, 10)\n"
     ]
    }
   ],
   "source": [
    "def conv_centralnet_test():\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    with tf.device(device):\n",
    "        #conv1\n",
    "        x1 = tf.placeholder(tf.float32)\n",
    "        conv1_w1 = tf.zeros((5, 5, 3, 6))\n",
    "        conv1_b1 = tf.zeros((6,))\n",
    "        conv1_w2 = tf.zeros((3, 3, 6, 9))\n",
    "        conv1_b2 = tf.zeros((9,))\n",
    "        fc1_w = tf.zeros((32 * 32 * 9, 10))\n",
    "        fc1_b = tf.zeros((10,))\n",
    "        feed1 = [conv1_w1, conv1_b1, conv1_w2, conv1_b2, fc1_w, fc1_b]\n",
    "        \n",
    "        #conv2\n",
    "        \n",
    "        x2 = tf.placeholder(tf.float32)\n",
    "        conv2_w1 = tf.zeros((5, 5, 3, 6))\n",
    "        conv2_b1 = tf.zeros((6,))\n",
    "        conv2_w2 = tf.zeros((3, 3, 6, 9))\n",
    "        conv2_b2 = tf.zeros((9,))\n",
    "        fc2_w = tf.zeros((32 * 32 * 9, 10))\n",
    "        fc2_b = tf.zeros((10,))\n",
    "        feed2 = [conv2_w1, conv2_b1, conv2_w2, conv2_b2, fc2_w, fc2_b]\n",
    "\n",
    "        \n",
    "        #centralnet\n",
    "        c11 = tf.zeros((32*32*6, 4))\n",
    "        c12 = tf.zeros((32*32*6, 4))\n",
    "        c21 = tf.zeros((32*32*9, 10))\n",
    "        c22 = tf.zeros((32*32*9, 10))\n",
    "        cc1 = tf.zeros((4, 10))\n",
    " \n",
    "        feedc=[c11,c12,c21,c22,cc1]\n",
    "\n",
    "        #final\n",
    "        \n",
    "        params=[feed1,feed2,feedc]\n",
    "        scores=conv_centralnet(x1,x2,params)\n",
    "    # Inputs to convolutional layers are 4-dimensional arrays with shape\n",
    "    # [batch_size, height, width, channels]\n",
    "    x1_np = np.zeros((64, 32, 32, 3))\n",
    "    x2_np = np.zeros((64, 32, 32, 3))   \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        scores_np = sess.run(scores, feed_dict={x1: x1_np,x2:x2_np})\n",
    "        print('scores_np has shape: ', scores_np.shape)\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    conv_centralnet_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,) int32\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n",
      "0 (64, 32, 32, 3) (64,)\n",
      "1 (64, 32, 32, 3) (64,)\n",
      "2 (64, 32, 32, 3) (64,)\n",
      "3 (64, 32, 32, 3) (64,)\n",
      "4 (64, 32, 32, 3) (64,)\n",
      "5 (64, 32, 32, 3) (64,)\n",
      "6 (64, 32, 32, 3) (64,)\n"
     ]
    }
   ],
   "source": [
    "#准备数据\n",
    "\n",
    "def load_cifar10(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Fetch the CIFAR-10 dataset from the web and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 dataset and use appropriate data types and shapes\n",
    "    cifar10 = tf.keras.datasets.cifar10.load_data()\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.int32).flatten()\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "    y_test = np.asarray(y_test, dtype=np.int32).flatten()\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean pixel and divide by std\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "NHW = (0, 1, 2)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
    "\n",
    "\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=64)\n",
    "\n",
    "# We can iterate through a dataset like this:\n",
    "for t, (x, y) in enumerate(train_dset):\n",
    "    print(t, x.shape, y.shape)\n",
    "    if t > 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(scores, y, params, learning_rate):\n",
    "    \"\"\"\n",
    "    Set up the part of the computational graph which makes a training step.\n",
    "\n",
    "    Inputs:\n",
    "    - scores: TensorFlow Tensor of shape (N, C) giving classification scores for\n",
    "      the model.\n",
    "    - y: TensorFlow Tensor of shape (N,) giving ground-truth labels for scores;\n",
    "      y[i] == c means that c is the correct class for scores[i].\n",
    "    - params: List of TensorFlow Tensors giving the weights of the model\n",
    "    - learning_rate: Python scalar giving the learning rate to use for gradient\n",
    "      descent step.\n",
    "      \n",
    "    Returns:\n",
    "    - loss: A TensorFlow Tensor of shape () (scalar) giving the loss for this\n",
    "      batch of data; evaluating the loss also performs a gradient descent step\n",
    "      on params (see above).\n",
    "    \"\"\"\n",
    "    #展开列表\n",
    "    feed= [i for k in params for i in k]\n",
    "    # First compute the loss; the first line gives losses for each example in\n",
    "    # the minibatch, and the second averages the losses across the batch\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "\n",
    "    # Compute the gradient of the loss with respect to each parameter of the the\n",
    "    # network. This is a very magical function call: TensorFlow internally\n",
    "    # traverses the computational graph starting at loss backward to each element\n",
    "    # of params, and uses backpropagation to figure out how to compute gradients;\n",
    "    # it then adds new operations to the computational graph which compute the\n",
    "    # requested gradients, and returns a list of TensorFlow Tensors that will\n",
    "    # contain the requested gradients when evaluated.\n",
    "    grad_params = tf.gradients(loss, feed)\n",
    "    \n",
    "    # Make a gradient descent step on all of the model parameters.\n",
    "    new_weights = []   \n",
    "    for w, grad_w in zip(feed, grad_params):\n",
    "        new_w = tf.assign_sub(w, learning_rate * grad_w)\n",
    "        new_weights.append(new_w)\n",
    "\n",
    "    # Insert a control dependency so that evaluting the loss causes a weight\n",
    "    # update to happen; see the discussion above.\n",
    "    with tf.control_dependencies(new_weights):\n",
    "        return tf.identity(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part2(model_fn, init_fn, learning_rate):\n",
    "\n",
    "    # First clear the default graph\n",
    "    tf.reset_default_graph()\n",
    "    is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "    # Set up the computational graph for performing forward and backward passes,\n",
    "    # and weight updates.\n",
    "    with tf.device(device):\n",
    "        # Set up placeholders for the data and labels\n",
    "        x1 = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        x2 = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        \n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        params = init_fn()           # Initialize the model parameters\n",
    "        scores = model_fn(x1,x2, params) # Forward pass of the model\n",
    "        loss = training_step(scores, y, params, learning_rate)\n",
    "\n",
    "    # Now we actually run the graph many times using the training data\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize variables that will live in the graph\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for t, (x_np, y_np) in enumerate(train_dset):\n",
    "            # Run the graph on a batch of training data; recall that asking\n",
    "            # TensorFlow to evaluate loss will cause an SGD step to happen.\n",
    "            feed_dict = {x1: x_np,x2:x_np, y: y_np}\n",
    "            loss_np = sess.run(loss, feed_dict=feed_dict)\n",
    "            \n",
    "            # Periodically print the loss and check accuracy on the val set\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss_np))\n",
    "                check_accuracy(sess, val_dset, x1,x2, scores, is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(sess, dset, x1,x2, scores, is_training=None):\n",
    "    \"\"\"\n",
    "    Check accuracy on a classification model.\n",
    "    \n",
    "    Inputs:\n",
    "    - sess: A TensorFlow Session that will be used to run the graph\n",
    "    - dset: A Dataset object on which to check accuracy\n",
    "    - x: A TensorFlow placeholder Tensor where input images should be fed\n",
    "    - scores: A TensorFlow Tensor representing the scores output from the\n",
    "      model; this is the Tensor we will ask TensorFlow to evaluate.\n",
    "      \n",
    "    Returns: Nothing, but prints the accuracy of the model\n",
    "    \"\"\"\n",
    "    num_correct, num_samples = 0, 0\n",
    "    for x_batch, y_batch in dset:\n",
    "        feed_dict = {x1: x_batch,x2:x_batch, is_training: 0}\n",
    "        scores_np = sess.run(scores, feed_dict=feed_dict)\n",
    "        y_pred = scores_np.argmax(axis=1)\n",
    "        num_samples += x_batch.shape[0]\n",
    "        num_correct += (y_pred == y_batch).sum()\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_normal(shape):\n",
    "    if len(shape) == 2:\n",
    "        fan_in, fan_out = shape[0], shape[1]\n",
    "    elif len(shape) == 4:\n",
    "        fan_in, fan_out = np.prod(shape[:3]), shape[3]\n",
    "    return tf.random_normal(shape) * np.sqrt(2.0 / fan_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_centralnet_init():\n",
    "    \"\"\"\n",
    "    Initialize the weights of a Three-Layer ConvNet, for use with the\n",
    "    three_layer_convnet function defined above.\n",
    "    \n",
    "    Inputs: None\n",
    "    \n",
    "    Returns a list containing:\n",
    "    - conv_w1: TensorFlow Variable giving weights for the first conv layer\n",
    "    - conv_b1: TensorFlow Variable giving biases for the first conv layer\n",
    "    - conv_w2: TensorFlow Variable giving weights for the second conv layer\n",
    "    - conv_b2: TensorFlow Variable giving biases for the second conv layer\n",
    "    - fc_w: TensorFlow Variable giving weights for the fully-connected layer\n",
    "    - fc_b: TensorFlow Variable giving biases for the fully-connected layer\n",
    "    \"\"\"\n",
    "    params = None\n",
    "    #conv1\n",
    "    conv1_w1 = tf.Variable(kaiming_normal([5,5,3,32]))\n",
    "    conv1_b1 = tf.Variable(tf.zeros(32))\n",
    "    conv1_w2 = tf.Variable(kaiming_normal([3,3,32,16]))\n",
    "    conv1_b2 = tf.Variable(tf.zeros(16))\n",
    "    fc1_w = tf.Variable(kaiming_normal((16 * 32 * 32, 10)))\n",
    "    fc1_b = tf.Variable(tf.zeros(10))\n",
    "    #conv2\n",
    "    conv2_w1 = tf.Variable(kaiming_normal([5,5,3,32]))\n",
    "    conv2_b1 = tf.Variable(tf.zeros(32))\n",
    "    conv2_w2 = tf.Variable(kaiming_normal([3,3,32,16]))\n",
    "    conv2_b2 = tf.Variable(tf.zeros(16))\n",
    "    fc2_w = tf.Variable(kaiming_normal((16 * 32 * 32, 10)))\n",
    "    fc2_b = tf.Variable(tf.zeros(10))\n",
    "    #centralnet\n",
    "    c11 = tf.Variable(kaiming_normal((32 * 32 * 32, 4)))\n",
    "    c12 = tf.Variable(kaiming_normal((32 * 32 * 32, 4)))\n",
    "    c21 = tf.Variable(kaiming_normal((16 * 32 * 32, 10)))\n",
    "    c22 = tf.Variable(kaiming_normal((16 * 32 * 32, 10)))\n",
    "    cc1 = tf.Variable(kaiming_normal((4, 10)))\n",
    "    \n",
    "    feed1 = [conv1_w1, conv1_b1, conv1_w2, conv1_b2, fc1_w, fc1_b]\n",
    "    feed2 = [conv2_w1, conv2_b1, conv2_w2, conv2_b2, fc2_w, fc2_b]\n",
    "    feedc=[c11,c12,c21,c22,cc1]\n",
    "    params=[feed1,feed2,feedc]\n",
    "    \n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<tf.Variable 'Variable:0' shape=(5, 5, 3, 32) dtype=float32_ref>,\n",
       "  <tf.Variable 'Variable_1:0' shape=(32,) dtype=float32_ref>,\n",
       "  <tf.Variable 'Variable_2:0' shape=(3, 3, 32, 16) dtype=float32_ref>,\n",
       "  <tf.Variable 'Variable_3:0' shape=(16,) dtype=float32_ref>,\n",
       "  <tf.Variable 'Variable_4:0' shape=(16384, 10) dtype=float32_ref>,\n",
       "  <tf.Variable 'Variable_5:0' shape=(10,) dtype=float32_ref>],\n",
       " [<tf.Variable 'Variable_6:0' shape=(5, 5, 3, 32) dtype=float32_ref>,\n",
       "  <tf.Variable 'Variable_7:0' shape=(32,) dtype=float32_ref>,\n",
       "  <tf.Variable 'Variable_8:0' shape=(3, 3, 32, 16) dtype=float32_ref>,\n",
       "  <tf.Variable 'Variable_9:0' shape=(16,) dtype=float32_ref>,\n",
       "  <tf.Variable 'Variable_10:0' shape=(16384, 10) dtype=float32_ref>,\n",
       "  <tf.Variable 'Variable_11:0' shape=(10,) dtype=float32_ref>],\n",
       " [<tf.Variable 'Variable_12:0' shape=(32768, 4) dtype=float32_ref>,\n",
       "  <tf.Variable 'Variable_13:0' shape=(32768, 4) dtype=float32_ref>,\n",
       "  <tf.Variable 'Variable_14:0' shape=(16384, 10) dtype=float32_ref>,\n",
       "  <tf.Variable 'Variable_15:0' shape=(16384, 10) dtype=float32_ref>,\n",
       "  <tf.Variable 'Variable_16:0' shape=(4, 10) dtype=float32_ref>]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_centralnet_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 5.1540\n",
      "Got 103 / 1000 correct (10.30%)\n",
      "Iteration 100, loss = 3.3509\n",
      "Got 156 / 1000 correct (15.60%)\n",
      "Iteration 200, loss = 3.1254\n",
      "Got 171 / 1000 correct (17.10%)\n",
      "Iteration 300, loss = 3.1145\n",
      "Got 190 / 1000 correct (19.00%)\n",
      "Iteration 400, loss = 2.7905\n",
      "Got 204 / 1000 correct (20.40%)\n",
      "Iteration 500, loss = 3.0036\n",
      "Got 217 / 1000 correct (21.70%)\n",
      "Iteration 600, loss = 3.0157\n",
      "Got 234 / 1000 correct (23.40%)\n",
      "Iteration 700, loss = 2.5265\n",
      "Got 238 / 1000 correct (23.80%)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-5\n",
    "train_part2(conv_centralnet, conv_centralnet_init, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
